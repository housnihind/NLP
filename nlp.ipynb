{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea22fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9ad3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ee3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f4736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538dadf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81be2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40997633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ecddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d219ec90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m Text\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m tokens\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[43mtext\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(token)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "Text=(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe270b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "week\n",
      ",\n",
      "the\n",
      "University\n",
      "of\n",
      "Cambridge\n",
      "shared\n",
      "its\n",
      "own\n",
      "research\n",
      "that\n",
      "shows\n",
      "if\n",
      "everyone\n",
      "wears\n",
      "a\n",
      "mask\n",
      "outside\n",
      "home\n",
      ",\n",
      "dreaded\n",
      "‘\n",
      "second\n",
      "wave\n",
      "’\n",
      "of\n",
      "the\n",
      "pandemic\n",
      "can\n",
      "be\n",
      "avoided\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text=(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3226815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hind\n",
      "hh\n",
      "HOUSNI\n",
      "tes\n",
      ".\n",
      ",\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(\"Hind hh HOUSNI tes . , test\")\n",
    "for token in tokens :\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "952b30f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives.',\n",
       " 'Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others.',\n",
       " 'Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels?',\n",
       " 'Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves.',\n",
       " 'This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g.',\n",
       " 'radio, movies, television, the internet, mobiles) and the zeitgeist (e.g.',\n",
       " 'cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens.',\n",
       " 'Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18493d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello,i'm not free this after noon ,lert's go tomorrow\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello,i'm not free this after noon ,lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7424f55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello.i'm not free this after noon .lert's go tomorrow\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello.i'm not free this after noon .lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a368c747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello.', \"i'm not free this after noon.\", \"lert's go tomorrow\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello. i'm not free this after noon. lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31a3c24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "text=\"I love spring season. I go hiking with my friends\"\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d0a126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe1bb538dc94ec69c16e6ef6d7af5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee90657c8644e7b7b2b77c7d92140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c0a4c7a2d4779896af0a723c4244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d50a178e70412a8a62c4461a9c8b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceb1f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3500, 2161, 1012, 1045, 2175, 13039, 2007, 2026, 2814, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] i love spring season. i go hiking with my friends [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer.encode(text)\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caf27992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 17666, 7570, 2271, 3490, 3231, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] hind housni test [SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer.encode(\"hind housni test\")\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883e4f0",
   "metadata": {},
   "source": [
    "<h2> tokenize text with stopwords </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30e62c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walter',\n",
       " 'feeling anxious',\n",
       " 'He',\n",
       " 'diagnosed today',\n",
       " 'He probably',\n",
       " 'best person I know']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\n",
    "stop_words_and_delims =['was','is','the',',','.','-','?']\n",
    "for r in stop_words_and_delims:\n",
    "    text=text.replace(r,'DELIM')\n",
    "WORDS = [t.strip() for t in text.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a: a not in [''], words))\n",
    "words_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35de7736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked beh',\n",
       " 'd',\n",
       " 'door',\n",
       " \"nd didn't like\",\n",
       " 'he s',\n",
       " 'w. The three-ye',\n",
       " 'r-old girl r',\n",
       " 'n down',\n",
       " 'be',\n",
       " 'ch',\n",
       " 's',\n",
       " 'kite flew beh',\n",
       " 'd her. R',\n",
       " 'ndom words',\n",
       " 'front',\n",
       " 'o',\n",
       " 'r r',\n",
       " 'ndom words cre',\n",
       " 'te',\n",
       " 'r',\n",
       " 'ndom sentence.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\"in\",\"in\",\"of\",\"a\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3810cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " 'door',\n",
       " \"nd didn't like\",\n",
       " 'he s',\n",
       " 'w. The three-ye',\n",
       " 'r-old girl r',\n",
       " 'n down',\n",
       " 'be',\n",
       " 'ch',\n",
       " 's',\n",
       " 'kite flew behind her. R',\n",
       " 'ndom words in front of o',\n",
       " 'r r',\n",
       " 'ndom words cre',\n",
       " 'te',\n",
       " 'r',\n",
       " 'ndom sentence.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\"a\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1784225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " \"door and didn't like\",\n",
       " 'he saw. The three-year-old girl ran down',\n",
       " 'beach as',\n",
       " 'kite flew behind her. Random words in front of o',\n",
       " 'r random words create a random sentence.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "353947c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " \"door and didn't like\",\n",
       " 'he saw',\n",
       " 'The three-year-old girl ran down',\n",
       " 'beach as',\n",
       " 'kite flew behind her',\n",
       " 'Random words in front of o',\n",
       " 'r random words create',\n",
       " 'a random sentence']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create. a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\".\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70579e0f",
   "metadata": {},
   "source": [
    "<h2>remove stop words in a text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a771939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outbreak coronavirus disease 2019 ( COVID-19 ) created global health crisis deep impact way perceive world everyday lives . Not rate contagion patterns transmission threatens sense agency , safety measures put place contain spread virus also require social distancing refraining inherently human , find solace company others . Within context physical threat , social physical distancing , well public alarm , ( ) role different mass media channels lives individual , social societal levels ? Mass media long recognized powerful forces shaping experience world . This recognition accompanied growing volume research , closely follows footsteps technological transformations ( e.g . radio , movies , television , internet , mobiles ) zeitgeist ( e.g . cold war , 9/11 , climate change ) attempt map mass media major impacts perceive , individuals citizens . Are media ( broadcast digital ) still able convey sense unity reaching large audiences , messages lost noisy crowd mass self-communication ?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "my_stopwords=set(stopwords.words('english'))\n",
    "new_tokens=[]\n",
    "text=\"\"\"the outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "all_tokens=nltk.word_tokenize(text)\n",
    "\n",
    "for token in all_tokens:\n",
    "  if token not in my_stopwords:\n",
    "    new_tokens.append(token)\n",
    "\n",
    "\n",
    "\" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48319284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The You bite lower jaw . I think I buy red car , I lease blue one . The mysterious diary records voice .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "stopw = set(stopwords.words('english'))\n",
    "new=[]\n",
    "test=\"The You bite up because of your lower jaw. I think I will buy the red car, or I will lease the blue one. The mysterious diary records the voice.\"\n",
    "tokens=nltk.word_tokenize(test)\n",
    "for token  in tokens :\n",
    " if token not in stopw:\n",
    "     new.append(token)\n",
    "        \n",
    "        \n",
    "\" \".join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb681398",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fead614",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m new_tokens\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m----> 6\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_stop\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     new_tokens\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(new_tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_stop'"
     ]
    }
   ],
   "source": [
    "text=\"\"\"the outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "import nltk \n",
    "doc=(text)\n",
    "new_tokens=[]\n",
    "for token in doc:\n",
    "  if token.is_stop==False:\n",
    "    new_tokens.append(token.text)\n",
    "\n",
    "\" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c646791",
   "metadata": {},
   "source": [
    "<h2>remove punctuations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9193bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The match has concluded India has won the match Will we fin the finals too'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text=\"The match has concluded !!! India has won the match . Will we fin the finals too ? !\"\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efce261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tart lemonade quenched her thirst but not her longing She moved forward only because she trusted that the ending she now was going through must be followed by a new beginning Pair your designer cowboy hat with scuba gear for a memorable occasion'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"The tart lemonade quenched her thirst, but not her longing.She moved forward only because she trusted that the ending she now was going through must be followed by a new beginning.Pair your designer cowboy hat with scuba gear for a memorable occasion.\"\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(test)\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d87fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The__tart__lemonade__quenched__her__thirst__but__not__her__longing__She__moved__forward__only__because__she__trusted__that__the__ending__she__now__was__going__through__must__be__followed__by__a__new__beginning__Pair__your__designer__cowboy__hat__with__scuba__gear__for__a__memorable__occasion'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"The tart lemonade quenched her thirst, but not her longing.She moved forward only because she trusted that the ending she now was going through must be followed by a new beginning.Pair your designer cowboy hat with scuba gear for a memorable occasion.\"\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(test)\n",
    "\"__\".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f38e8d",
   "metadata": {},
   "source": [
    "<h2>perform stemming</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1551a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc is an art . student should be taught danc as a subject in school . i danc in mani of my school function . some peopl are alway hesit to danc .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
    "from nltk.stem import PorterStemmer \n",
    "stemmer=PorterStemmer()\n",
    "stemmed_tokens=[]\n",
    "for token in nltk.word_tokenize(text):\n",
    "  stemmed_tokens.append(stemmer.stem(token))\n",
    "\n",
    "\" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46328a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sarah ran from the serial killer hold a jug of milk.it is n't difficult to do a handstand if you just stand on your hands.i love eat toast chees and tuna sandwich .\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"Sarah ran from the serial killer holding a jug of milk.It isn't difficult to do a handstand if you just stand on your hands.I love eating toasted cheese and tuna sandwiches.\"\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmed=[]\n",
    "for token in nltk.word_tokenize(test):\n",
    "    stemmed.append(stemmer.stem(token))\n",
    "    \n",
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11728da8",
   "metadata": {},
   "source": [
    "<h2>lemmatize a given text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a916354",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "\n",
    "lemmatized=[token.lemma_ for token in doc]\n",
    "\" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a2bbc",
   "metadata": {},
   "source": [
    "<h1>extract usernames from emails</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2bea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potter709', 'elixir101', 'granger111', 'severus77']\n"
     ]
    }
   ],
   "source": [
    "text= \"The new registrations are potter709@gmail.com , elixir101@gmail.com. If you find any disruptions, kindly contact granger111@gamil.com or severus77@gamil.com \"\n",
    "import re \n",
    "usernames= re.findall('(\\S+)@', text)     \n",
    "print(usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e69bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hindhousni25', ',hindhousni304']\n"
     ]
    }
   ],
   "source": [
    "test=\"hindhousni25@gmail.com ,hindhousni304@gmail.com\"\n",
    "import re\n",
    "name=re.findall('(\\S+)@',test)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8fb09",
   "metadata": {},
   "source": [
    "<h2>finding the most common words in the text excluding stopwords</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2aceaa",
   "metadata": {},
   "source": [
    "<h2>spell correction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d7e5d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHe is a gret person. He beleives in bod\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      3\u001b[0m text\u001b[38;5;241m=\u001b[39mTextBlob(text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(text\u001b[38;5;241m.\u001b[39mcorrect())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "text=\"He is a gret person. He beleives in bod\"\n",
    "from textblob import TextBlob\n",
    "text=TextBlob(text)\n",
    "print(text.correct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a00a3e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textblob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtextblob\u001b[49m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textblob' is not defined"
     ]
    }
   ],
   "source": [
    "textblob.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a13187",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textblob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtextblob\u001b[49m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textblob' is not defined"
     ]
    }
   ],
   "source": [
    "textblob.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b384f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a great person. He believes in god\n"
     ]
    }
   ],
   "source": [
    "text=\"He is a great person. He believes in god\"\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "text=TextBlob(text)\n",
    "print(text.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84cba7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Felo my name is hind ad i m eighteen years old\n"
     ]
    }
   ],
   "source": [
    "test=\"Helo my name is hind ad i m eighteen years old\"\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "test=TextBlob(test)\n",
    "print(test.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce063728",
   "metadata": {},
   "source": [
    "<h1>tokenize tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bff63fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'goa',\n",
       " 'vaction',\n",
       " 'summervacation',\n",
       " 'Fancy',\n",
       " 'dinner',\n",
       " 'Beachbay',\n",
       " 'restro']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\" Having lots of fun #goa #vaction #summervacation. Fancy dinner @Beachbay restro :) \"\n",
    "import re\n",
    "text=re.sub(r'[^\\w]',' ',text)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer=TweetTokenizer()\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b97ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test  tweets  big\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test', 'tweets', 'big']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"#test #tweets #big\"\n",
    "import re\n",
    "test=re.sub(r'[^\\w]',' ',test)\n",
    "print(test)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer=TweetTokenizer()\n",
    "tokenizer.tokenize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12a248",
   "metadata": {},
   "source": [
    "<h1>extract all the nouns in a text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be67675",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e70be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJames works at Microsoft. She lives in manchester and likes to play the flute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m nlp\u001b[38;5;241m=\u001b[39m\u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "text=\"James works at Microsoft. She lives in manchester and likes to play the flute\"\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc98c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abd9fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m word2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterrible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m word3\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcellent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m spacy download en_core_web_lg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_lg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "word1=\"amazing\"\n",
    "word2=\"terrible\"\n",
    "word3=\"excellent\"\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "token1=nlp(word1)\n",
    "token2=nlp(word2)\n",
    "token3=nlp(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b536f37f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vectorization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvectorization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vectorizer\n\u001b[0;32m      5\u001b[0m text1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaj Mahal is a tourist place in India\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m text2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreat Wall of China is a tourist place in china\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vectorization'"
     ]
    }
   ],
   "source": [
    "import vectorize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from vectorization import Vectorizer\n",
    "text1='Taj Mahal is a tourist place in India'\n",
    "text2='Great Wall of China is a tourist place in china'\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "matrix=vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323bd6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e44304",
   "metadata": {},
   "source": [
    "<h2>find the cosine similarity of two documents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27391f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.45584231]\n",
      " [0.45584231 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "text1='Taj Mahal is a tourist place in India'\n",
    "text2='Great Wall of China is a tourist place in china'\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f4a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "text1=\"HOUSNI hind\"\n",
    "text2=\"HOUSNI hind\"\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e55340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "text1=\"HOUSNI hind \"\n",
    "text2=\"HOUSNI hind\"\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341ae01",
   "metadata": {},
   "source": [
    "<h2>find soft cosine similarity of documents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d9dc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m     16\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary([simple_preprocess(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents])\n\u001b[0;32m     17\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m fasttext_model300\u001b[38;5;241m.\u001b[39msimilarity_matrix(dictionary, tfidf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, exponent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, nonzero_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'apps'"
     ]
    }
   ],
   "source": [
    "doc_soup = \"Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), that is made by combining ingredients of meat or vegetables with stock, juice, water, or another liquid. \"\n",
    "\n",
    "doc_noodles = \"Noodles are a staple food in many cultures. They are made from unleavened dough which is stretched, extruded, or rolled flat and cut into one of a variety of shapes.\"\n",
    "\n",
    "doc_dosa = \"Dosa is a type of pancake from the Indian subcontinent, made from a fermented batter. It is somewhat similar to a crepe in appearance. Its main ingredients are rice and black gram.\"\n",
    "\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
    "\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "import gensim\n",
    "from apps.document.models import Document\n",
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "sent_1 = dictionary.doc2bow(simple_preprocess(doc_trump))\n",
    "sent_2 = dictionary.doc2bow(simple_preprocess(doc_election))\n",
    "sent_3 = dictionary.doc2bow(simple_preprocess(doc_putin))\n",
    "sent_4 = dictionary.doc2bow(simple_preprocess(doc_soup))\n",
    "sent_5 = dictionary.doc2bow(simple_preprocess(doc_noodles))\n",
    "sent_6 = dictionary.doc2bow(simple_preprocess(doc_dosa))\n",
    "\n",
    "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e1e229",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: first byte timeout",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m word2vec_model300 \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword2vec-google-news-300\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m word2vec_model300\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\downloader.py:496\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    494\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dir, file_name)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(folder_dir):\n\u001b[1;32m--> 496\u001b[0m     \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\downloader.py:365\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    363\u001b[0m tmp_dir \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkdtemp()\n\u001b[0;32m    364\u001b[0m init_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 365\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_load_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m total_parts \u001b[38;5;241m=\u001b[39m _get_parts(name)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_parts \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:239\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    240\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:555\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    553\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    554\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 555\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:747\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    744\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    745\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:523\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    522\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 523\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:632\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 632\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:561\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:641\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: first byte timeout"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "word2vec_model300.most_similar('amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4607b",
   "metadata": {},
   "source": [
    "<h2>compute Word mover distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbf0b2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'wmdistance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m sentence_orange \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOranges are my favorite fruit\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m sent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapples are not my favorite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwmdistance\u001b[49m(sent, sentence_orange)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'wmdistance'"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec()\n",
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec()\n",
    "sentence_orange = 'Oranges are my favorite fruit'\n",
    "sent=\"apples are not my favorite\"\n",
    "distance = model.wmdistance(sent, sentence_orange)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f34a41",
   "metadata": {},
   "source": [
    "<h2>replace all the pronouns in a text with their respective object names</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cf4fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralcoref\n",
      "  Using cached neuralcoref-4.0.tar.gz (368 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from neuralcoref) (1.21.5)\n",
      "Requirement already satisfied: boto3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from neuralcoref) (1.21.32)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from neuralcoref) (2.27.1)\n",
      "Requirement already satisfied: spacy>=2.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from neuralcoref) (3.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (61.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (4.64.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.7.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.4.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (8.0.17)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.1.0->neuralcoref) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy>=2.1.0->neuralcoref) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.1.0->neuralcoref) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.1.0->neuralcoref) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=2.1.0->neuralcoref) (8.0.4)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from boto3->neuralcoref) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from boto3->neuralcoref) (1.24.32)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from boto3->neuralcoref) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.32->boto3->neuralcoref) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.1.0->neuralcoref) (2.0.1)\n",
      "Building wheels for collected packages: neuralcoref\n",
      "  Building wheel for neuralcoref (setup.py): started\n",
      "  Building wheel for neuralcoref (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for neuralcoref\n",
      "Failed to build neuralcoref\n",
      "Installing collected packages: neuralcoref\n",
      "    Running setup.py install for neuralcoref: started\n",
      "    Running setup.py install for neuralcoref: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\hp\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d8sax6m7\\\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d8sax6m7\\\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\hp\\AppData\\Local\\Temp\\pip-wheel-ng7qhbv5'\n",
      "       cwd: C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-d8sax6m7\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\n",
      "  Complete output (25 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\neuralcoref\n",
      "  copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-3.9\\neuralcoref\n",
      "  copying neuralcoref\\__init__.py -> build\\lib.win-amd64-3.9\\neuralcoref\n",
      "  creating build\\lib.win-amd64-3.9\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-3.9\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-3.9\\neuralcoref\\tests\n",
      "  creating build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "  running build_ext\n",
      "  building 'neuralcoref.neuralcoref' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for neuralcoref\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\hp\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d8sax6m7\\\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d8sax6m7\\\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\hp\\AppData\\Local\\Temp\\pip-record-sqf87_c_\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\hp\\anaconda3\\Include\\neuralcoref'\n",
      "         cwd: C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-d8sax6m7\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\n",
      "    Complete output (27 lines):\n",
      "    running install\n",
      "    C:\\Users\\hp\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "      warnings.warn(\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\neuralcoref\n",
      "    copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-3.9\\neuralcoref\n",
      "    copying neuralcoref\\__init__.py -> build\\lib.win-amd64-3.9\\neuralcoref\n",
      "    creating build\\lib.win-amd64-3.9\\neuralcoref\\tests\n",
      "    copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-3.9\\neuralcoref\\tests\n",
      "    copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-3.9\\neuralcoref\\tests\n",
      "    creating build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-3.9\\neuralcoref\\train\n",
      "    running build_ext\n",
      "    building 'neuralcoref.neuralcoref' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\hp\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d8sax6m7\\\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Temp\\\\pip-install-d8sax6m7\\\\neuralcoref_8ed4557bbf8145b293efeec64d788296\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\hp\\AppData\\Local\\Temp\\pip-record-sqf87_c_\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\hp\\anaconda3\\Include\\neuralcoref' Check the logs for full command output.\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m My sister has a dog and she loves him\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install neuralcoref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneuralcoref\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "text=\" My sister has a dog and she loves him\"\n",
    "!pip install neuralcoref\n",
    "import spacy\n",
    "import neuralcoref\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f337d",
   "metadata": {},
   "source": [
    "<h2>extract topic keywords using LSA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56803850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "learn new life travelling country feel things  \n",
      "Topic 1: \n",
      "life cherish diaries let share experience home  \n",
      "Topic 2: \n",
      "feel know time people just regions sure  \n",
      "Topic 3: \n",
      "time especially cherish diaries let share life  \n",
      "Topic 4: \n",
      "cherish diaries let share makes feel know  \n",
      "Topic 5: \n",
      "culture augustine course cultural cultures eyes habits  \n",
      "Topic 6: \n",
      "experiences want life things advantage abroad bad  \n",
      "Topic 7: \n",
      "observe feel experiences want skills test know  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts= [\"\"\"It's all about travel. I travel a lot.  those who do not travel read only a page.” – said Saint Augustine. He was a great travel person. Travelling can teach you more than any university course. You learn about the culture of the country you visit. If you talk to locals, you will likely learn about their thinking, habits, traditions and history as well.If you travel, you will not only learn about foreign cultures, but about your own as well. You will notice the cultural differences, and will find out what makes your culture unique. After retrurning from a long journey, you will see your country with new eyes.\"\"\",\n",
    "        \"\"\" You can learn a lot about yourself through travelling. You can observe how you feel beeing far from your country. You will find out how you feel about your homeland.You should travel You will realise how you really feel about foreign people. You will find out how much you know/do not know about the world. You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills. You will not be the same person after returning home.During travelling you will meet people that are very different from you. If you travel enough, you will learn to accept and appreciate these differences. Traveling makes you more open and accepting.\"\"\",\n",
    "        \"\"\"Some of my most cherished memories are from the times when I was travelling. If you travel, you can experience things that you could never experience at home. You may see beautiful places and landscapes that do not exist where you live. You may meet people that will change your life, and your thingking. You may try activities that you have never tried before.Travelling will inevitably make you more independent and confident. You will realise that you can cope with a lot of unexpected situations. You will realise that you can survive without all that help that is always available for you at home. You will likely find out that you are much stronger and braver than you have expected.\"\"\",\n",
    "        \"\"\"If you travel, you may learn a lot of useful things. These things can be anything from a new recepie, to a new, more effective solution to an ordinary problem or a new way of creating something.Even if you go to a country where they speak the same language as you, you may still learn some new words and expressions that are only used there. If you go to a country where they speak a different language, you will learn even more.\"\"\",\n",
    "        \"\"\"After arriving home from a long journey, a lot of travellers experience that they are much more motivated than they were before they left. During your trip you may learn things that you will want to try at home as well. You may want to test your new skills and knowledge. Your experiences will give you a lot of energy.During travelling you may experience the craziest, most exciting things, that will eventually become great stories that you can tell others. When you grow old and look back at your life and all your travel experiences, you will realise how much you have done in your life and your life was not in vain. It can provide you with happiness and satisfaction for the rest of your life.\"\"\",\n",
    "        \"\"\"The benefits of travel are not just a one-time thing: travel changes you physically and psychologically. Having little time or money isn't a valid excuse. You can travel for cheap very easily. If you have a full-time job and a family, you can still travel on the weekends or holidays, even with a baby. travel  more is likely to have a tremendous impact on your mental well-being, especially if you're no used to going out of your comfort zone. Trust me: travel more and your doctor will be happy. Be sure to get in touch with your physician, they might recommend some medication to accompany you in your travels, especially if you're heading to regions of the globe with potentially dangerous diseases.\"\"\",\n",
    "        \"\"\"Sure, you probably feel comfortable where you are, but that is just a fraction of the world! If you are a student, take advantage of programs such as Erasmus to get to know more people, experience and understand their culture. Dare traveling to regions you have a skeptical opinion about. I bet that you will change your mind and realize that everything is not so bad abroad.\"\"\",\n",
    "        \"\"\" So, travel makes you cherish life. Let's travel more . Share your travel diaries with us too\"\"\"\n",
    "        ]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "SVD_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "SVD_model.fit(matrix)\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(SVD_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0],end=' ')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b714996",
   "metadata": {},
   "source": [
    "<h2>extract topic keywords using NMF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed51434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('new', 0.6329770846997613), ('learn', 0.4981038982593185), ('speak', 0.4747754621454461), ('language', 0.4344302967047186), ('country', 0.36653909845383154), ('things', 0.3433223730439047)]\n",
      "Topic 1:\n",
      "[('life', 0.34063551920788804), ('home', 0.31402014643240755), ('experience', 0.3025841622571289), ('realise', 0.24642870225288363), ('travelling', 0.2180915553025079), ('things', 0.20763478958892617)]\n",
      "Topic 2:\n",
      "[('feel', 0.34624840139223845), ('know', 0.2840008818200803), ('people', 0.24312668835450774), ('world', 0.22169277349692007), ('traveling', 0.22169277349692007), ('bet', 0.18671974365540414)]\n",
      "Topic 3:\n",
      "[('time', 0.4416317319305317), ('especially', 0.2944211546203545), ('zone', 0.14721057731017725), ('dangerous', 0.14721057731017725), ('excuse', 0.14721057731017725), ('benefits', 0.14721057731017725)]\n",
      "Topic 4:\n",
      "[('cherish', 0.470371391001751), ('diaries', 0.470371391001751), ('share', 0.470371391001751), ('let', 0.470371391001751), ('life', 0.35360092337281224), ('makes', 0.34102862453448896)]\n",
      "Topic 5:\n",
      "[('learn', 0.279059600110251), ('culture', 0.2285890660745814), ('country', 0.20863153116871358), ('locals', 0.14082722468160694), ('eyes', 0.14082722468160694), ('retrurning', 0.14082722468160694)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts= [\"\"\"It's all about travel. I travel a lot.  those who do not travel read only a page.” – said Saint Augustine. He was a great travel person. Travelling can teach you more than any university course. You learn about the culture of the country you visit. If you talk to locals, you will likely learn about their thinking, habits, traditions and history as well.If you travel, you will not only learn about foreign cultures, but about your own as well. You will notice the cultural differences, and will find out what makes your culture unique. After retrurning from a long journey, you will see your country with new eyes.\"\"\",\n",
    "        \"\"\" You can learn a lot about yourself through travelling. You can observe how you feel beeing far from your country. You will find out how you feel about your homeland.You should travel You will realise how you really feel about foreign people. You will find out how much you know/do not know about the world. You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills. You will not be the same person after returning home.During travelling you will meet people that are very different from you. If you travel enough, you will learn to accept and appreciate these differences. Traveling makes you more open and accepting.\"\"\",\n",
    "        \"\"\"Some of my most cherished memories are from the times when I was travelling. If you travel, you can experience things that you could never experience at home. You may see beautiful places and landscapes that do not exist where you live. You may meet people that will change your life, and your thingking. You may try activities that you have never tried before.Travelling will inevitably make you more independent and confident. You will realise that you can cope with a lot of unexpected situations. You will realise that you can survive without all that help that is always available for you at home. You will likely find out that you are much stronger and braver than you have expected.\"\"\",\n",
    "        \"\"\"If you travel, you may learn a lot of useful things. These things can be anything from a new recepie, to a new, more effective solution to an ordinary problem or a new way of creating something.Even if you go to a country where they speak the same language as you, you may still learn some new words and expressions that are only used there. If you go to a country where they speak a different language, you will learn even more.\"\"\",\n",
    "        \"\"\"After arriving home from a long journey, a lot of travellers experience that they are much more motivated than they were before they left. During your trip you may learn things that you will want to try at home as well. You may want to test your new skills and knowledge. Your experiences will give you a lot of energy.During travelling you may experience the craziest, most exciting things, that will eventually become great stories that you can tell others. When you grow old and look back at your life and all your travel experiences, you will realise how much you have done in your life and your life was not in vain. It can provide you with happiness and satisfaction for the rest of your life.\"\"\",\n",
    "        \"\"\"The benefits of travel are not just a one-time thing: travel changes you physically and psychologically. Having little time or money isn't a valid excuse. You can travel for cheap very easily. If you have a full-time job and a family, you can still travel on the weekends or holidays, even with a baby. travel  more is likely to have a tremendous impact on your mental well-being, especially if you're no used to going out of your comfort zone. Trust me: travel more and your doctor will be happy. Be sure to get in touch with your physician, they might recommend some medication to accompany you in your travels, especially if you're heading to regions of the globe with potentially dangerous diseases.\"\"\",\n",
    "        \"\"\"Sure, you probably feel comfortable where you are, but that is just a fraction of the world! If you are a student, take advantage of programs such as Erasmus to get to know more people, experience and understand their culture. Dare traveling to regions you have a skeptical opinion about. I bet that you will change your mind and realize that everything is not so bad abroad.\"\"\",\n",
    "        \"\"\" So, travel makes you cherish life. Let's travel more . Share your travel diaries with us too\"\"\"\n",
    "        ]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "nmf_matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=6)\n",
    "nmf_model.fit(nmf_matrix)\n",
    "def print_topics_nmf(model, vectorizer, top_n=6):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "print_topics_nmf(nmf_model,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b1fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('used', 0.447213595499958), ('sunlight', 0.447213595499958), ('photography', 0.447213595499958), ('flash', 0.447213595499958)]\n",
      "Topic 1:\n",
      "[('pancakes', 0.447213595499958), ('ketchup', 0.447213595499958), ('joyce', 0.447213595499958), ('enjoyed', 0.447213595499958)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts= [\"Joyce enjoyed eating pancakes with ketchup.\",\"Flash photography is best used in full sunlight.\"]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "nmf_matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=2)\n",
    "nmf_model.fit(nmf_matrix)\n",
    "def print_topics_nmf(model, vectorizer, top_n=4):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "print_topics_nmf(nmf_model,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0373f262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('worn', 0.3451565284569502), ('shouldn', 0.3451565284569502)]\n",
      "Topic 1:\n",
      "[('sundays', 0.5999094080149103), ('home', 0.5999094080149103)]\n",
      "Topic 2:\n",
      "[('ketchup', 0.447213595499958), ('eating', 0.447213595499958)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts= [\"Joyce enjoyed eating pancakes with ketchup.\",\"Flash photography is best used in full sunlight.\",\"Some bathing suits just shouldn’t be worn by some people.\",\"I am never at home on Sundays.\"]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "nmf_matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=3)\n",
    "nmf_model.fit(nmf_matrix)\n",
    "def print_topics_nmf(model, vectorizer, top_n=2):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "print_topics_nmf(nmf_model,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ddde5",
   "metadata": {},
   "source": [
    "<h2>extract topic keywords using NMF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057e2517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('new', 0.6329770846997628), ('learn', 0.4981038982593197), ('speak', 0.4747754621454472), ('language', 0.4344302967047196), ('country', 0.3665390984538324), ('things', 0.34332237304390545)]\n",
      "Topic 1:\n",
      "[('life', 0.3406355192078854), ('home', 0.3140201464324049), ('experience', 0.3025841622571263), ('realise', 0.24642870225288152), ('travelling', 0.21809155530250607), ('things', 0.20763478958892448)]\n",
      "Topic 2:\n",
      "[('feel', 0.3462484013922405), ('know', 0.28400088182008193), ('people', 0.24312668835450918), ('world', 0.22169277349692137), ('traveling', 0.22169277349692137), ('bet', 0.1867197436554053)]\n",
      "Topic 3:\n",
      "[('time', 0.4416317319305339), ('especially', 0.294421154620356), ('zone', 0.147210577310178), ('dangerous', 0.147210577310178), ('excuse', 0.147210577310178), ('benefits', 0.147210577310178)]\n",
      "Topic 4:\n",
      "[('cherish', 0.47037139100175085), ('diaries', 0.47037139100175085), ('share', 0.47037139100175085), ('let', 0.47037139100175085), ('life', 0.353600923372812), ('makes', 0.34102862453448884)]\n",
      "Topic 5:\n",
      "[('learn', 0.2790596001102525), ('culture', 0.22858906607458268), ('country', 0.20863153116871472), ('locals', 0.14082722468160772), ('eyes', 0.14082722468160772), ('retrurning', 0.14082722468160772)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts= [\"\"\"It's all about travel. I travel a lot.  those who do not travel read only a page.” – said Saint Augustine. He was a great travel person. Travelling can teach you more than any university course. You learn about the culture of the country you visit. If you talk to locals, you will likely learn about their thinking, habits, traditions and history as well.If you travel, you will not only learn about foreign cultures, but about your own as well. You will notice the cultural differences, and will find out what makes your culture unique. After retrurning from a long journey, you will see your country with new eyes.\"\"\",\n",
    "        \"\"\" You can learn a lot about yourself through travelling. You can observe how you feel beeing far from your country. You will find out how you feel about your homeland.You should travel You will realise how you really feel about foreign people. You will find out how much you know/do not know about the world. You will be able to observe how you react in completely new situations. You will test your language, orientational and social skills. You will not be the same person after returning home.During travelling you will meet people that are very different from you. If you travel enough, you will learn to accept and appreciate these differences. Traveling makes you more open and accepting.\"\"\",\n",
    "        \"\"\"Some of my most cherished memories are from the times when I was travelling. If you travel, you can experience things that you could never experience at home. You may see beautiful places and landscapes that do not exist where you live. You may meet people that will change your life, and your thingking. You may try activities that you have never tried before.Travelling will inevitably make you more independent and confident. You will realise that you can cope with a lot of unexpected situations. You will realise that you can survive without all that help that is always available for you at home. You will likely find out that you are much stronger and braver than you have expected.\"\"\",\n",
    "        \"\"\"If you travel, you may learn a lot of useful things. These things can be anything from a new recepie, to a new, more effective solution to an ordinary problem or a new way of creating something.Even if you go to a country where they speak the same language as you, you may still learn some new words and expressions that are only used there. If you go to a country where they speak a different language, you will learn even more.\"\"\",\n",
    "        \"\"\"After arriving home from a long journey, a lot of travellers experience that they are much more motivated than they were before they left. During your trip you may learn things that you will want to try at home as well. You may want to test your new skills and knowledge. Your experiences will give you a lot of energy.During travelling you may experience the craziest, most exciting things, that will eventually become great stories that you can tell others. When you grow old and look back at your life and all your travel experiences, you will realise how much you have done in your life and your life was not in vain. It can provide you with happiness and satisfaction for the rest of your life.\"\"\",\n",
    "        \"\"\"The benefits of travel are not just a one-time thing: travel changes you physically and psychologically. Having little time or money isn't a valid excuse. You can travel for cheap very easily. If you have a full-time job and a family, you can still travel on the weekends or holidays, even with a baby. travel  more is likely to have a tremendous impact on your mental well-being, especially if you're no used to going out of your comfort zone. Trust me: travel more and your doctor will be happy. Be sure to get in touch with your physician, they might recommend some medication to accompany you in your travels, especially if you're heading to regions of the globe with potentially dangerous diseases.\"\"\",\n",
    "        \"\"\"Sure, you probably feel comfortable where you are, but that is just a fraction of the world! If you are a student, take advantage of programs such as Erasmus to get to know more people, experience and understand their culture. Dare traveling to regions you have a skeptical opinion about. I bet that you will change your mind and realize that everything is not so bad abroad.\"\"\",\n",
    "        \"\"\" So, travel makes you cherish life. Let's travel more . Share your travel diaries with us too\"\"\"\n",
    "        ]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "nmf_matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=6)\n",
    "nmf_model.fit(nmf_matrix)\n",
    "def print_topics_nmf(model, vectorizer, top_n=6):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "print_topics_nmf(nmf_model,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169d601e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('waited', 0.35355339059327395), ('trees', 0.35355339059327395), ('spark', 0.35355339059327395)]\n",
      "Topic 1:\n",
      "[('writing', 0.3779644730092272), ('way', 0.3779644730092272), ('track', 0.3779644730092272)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts=[\"Cursive writing is the best way to build a race track.\",\"The dead trees waited to be ignited by the smallest spark and seek their revenge.\"]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "nmf_matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=2)\n",
    "nmf_model.fit(nmf_matrix)\n",
    "def print_topics_nmf(model, vectorizer, top_n=3):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "print_topics_nmf(nmf_model,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d61dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('wall', 1.9902703539085524), ('day', 1.9902703539085524), ('different', 1.9902703539085524)]\n",
      "Topic 1:\n",
      "[('revenge', 0.3793599589565343), ('trees', 0.3793599589565343), ('dead', 0.3793599589565343)]\n",
      "Topic 2:\n",
      "[('campground', 0.3441374335683828), ('storage', 0.3441374335683828), ('seen', 0.3441374335683828)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "texts=[\"Cursive writing is the best way to build a race track.\",\"The dead trees waited to be ignited by the smallest spark and seek their revenge.\",\"More RVs were seen in the storage lot than at the campground.\",\"He had a wall full of masks so she could wear a different face every day.\"]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000,  max_df = 0.5, smooth_idf=True)\n",
    "nmf_matrix= vectorizer.fit_transform(texts)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf_model = NMF(n_components=3)\n",
    "nmf_model.fit(nmf_matrix)\n",
    "def print_topics_nmf(model, vectorizer, top_n=3):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        \n",
    "print_topics_nmf(nmf_model,vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d8be3",
   "metadata": {},
   "source": [
    "<h2>classify a text as positive/negative sentiment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ee1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.9533333333333333, subjectivity=1.0)\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "text=\"It was a very pleasant day\"\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038a1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "text=\"The snow-covered path was no help in finding his way out of the back-country.\"\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f90a2fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-0.5, subjectivity=1.0)\n"
     ]
    }
   ],
   "source": [
    "text=\"sad\"\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd0c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.8, subjectivity=1.0)\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "text=\"happy\"\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c152b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.5)\n"
     ]
    }
   ],
   "source": [
    "text=\"A glittering gem is not enough.\"\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78530a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.8, subjectivity=1.0)\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "text=\"happy\"\n",
    "from textblob import TextBlob\n",
    "blob=TextBlob(text)\n",
    "print(blob.sentiment)\n",
    "if(blob.sentiment.polarity > 0):\n",
    "  print(\"Positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306313d5",
   "metadata": {},
   "source": [
    "<h2>using the Word2Vec model for representing words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23124ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Word2Vec\n",
      "  Using cached word2vec-0.11.1.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from Word2Vec) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from Word2Vec) (1.21.5)\n",
      "Building wheels for collected packages: Word2Vec\n",
      "  Building wheel for Word2Vec (PEP 517): started\n",
      "  Building wheel for Word2Vec (PEP 517): finished with status 'error'\n",
      "Failed to build Word2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\hp\\anaconda3\\python.exe' 'C:\\Users\\hp\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\hp\\AppData\\Local\\Temp\\tmp3box58yp'\n",
      "       cwd: C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-d29yin_4\\word2vec_aebf67f8447d41e9b933fac6c9a60932\n",
      "  Complete output (99 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib\n",
      "  creating build\\lib\\word2vec\n",
      "  copying word2vec\\io.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\scripts_interface.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\utils.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\wordclusters.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\wordvectors.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\_generated_version.py -> build\\lib\\word2vec\n",
      "  copying word2vec\\__init__.py -> build\\lib\\word2vec\n",
      "  creating build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\test_core.py -> build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\test_import.py -> build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\test_scripts_present.py -> build\\lib\\word2vec\\tests\n",
      "  copying word2vec\\tests\\__init__.py -> build\\lib\\word2vec\\tests\n",
      "  running egg_info\n",
      "  writing word2vec.egg-info\\PKG-INFO\n",
      "  writing dependency_links to word2vec.egg-info\\dependency_links.txt\n",
      "  writing requirements to word2vec.egg-info\\requires.txt\n",
      "  writing top-level names to word2vec.egg-info\\top_level.txt\n",
      "  reading manifest file 'word2vec.egg-info\\SOURCES.txt'\n",
      "  adding license file 'LICENSE.txt'\n",
      "  writing manifest file 'word2vec.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-b2r395f8\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'word2vec.includes' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'word2vec.includes' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'word2vec.includes' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'word2vec.includes' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  C:\\Users\\hp\\AppData\\Local\\Temp\\pip-build-env-b2r395f8\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'word2vec.includes.win32' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'word2vec.includes.win32' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'word2vec.includes.win32' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'word2vec.includes.win32' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  creating build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\Makefile -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\compute-accuracy.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\distance.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word-analogy.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word2phrase.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word2vec-sentence2vec.c -> build\\lib\\word2vec\\includes\n",
      "  copying word2vec\\includes\\word2vec.c -> build\\lib\\word2vec\\includes\n",
      "  creating build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\Makefile -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\compute-accuracy.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\distance.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\win32-port.h -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\word-analogy.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\word2phrase.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  copying word2vec\\includes\\win32\\word2vec.c -> build\\lib\\word2vec\\includes\\win32\n",
      "  installing to build\\bdist.win-amd64\\wheel\n",
      "  running install\n",
      "  Running custom Install command\n",
      "  Compiling: gcc C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-d29yin_4\\word2vec_aebf67f8447d41e9b933fac6c9a60932\\word2vec\\includes\\win32/word2vec.c -o Scripts\\word2vec.exe -O2 -Wall -funroll-loops\n",
      "  error: [WinError 2] The system cannot find the file specified\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for Word2Vec\n",
      "ERROR: Could not build wheels for Word2Vec which use PEP 517 and cannot be installed directly\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m=\u001b[39mWord2Vec(all_tokens)\n\u001b[1;32m---> 17\u001b[0m vec \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model_w2v\u001b[38;5;241m.\u001b[39mwv[word]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, size))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vec' is not defined"
     ]
    }
   ],
   "source": [
    "texts= [\" Photography is an excellent hobby to pursue \",\n",
    "        \" Photographers usually develop patience, calmnesss\"\n",
    "        \" You can try Photography with any good mobile too\"]\n",
    "import nltk\n",
    "tokens=[]\n",
    "!pip install Word2Vec\n",
    "all_tokens=[]\n",
    "for text in texts:\n",
    "  tokens=[]\n",
    "  raw=nltk.word_tokenize(text)\n",
    "  for token in raw:\n",
    "    tokens.append(token)\n",
    "    all_tokens.append(tokens)\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec(all_tokens)\n",
    "vec += model_w2v.wv[word].reshape((1, size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc27c6",
   "metadata": {},
   "source": [
    "<h2>visualizing the word embedding obtained from word2Vec model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4068a02e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot\n\u001b[1;32m---> 22\u001b[0m X \u001b[38;5;241m=\u001b[39m model[\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m]\n\u001b[0;32m     23\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     24\u001b[0m result \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:661\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse KeyedVector\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    665\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "texts= [\" Photography is an excellent hobby to pursue \",\n",
    "        \" Photographers usually develop patience, calmnesss\"\n",
    "        \" You can try Photography with any good mobile too\"]\n",
    "for text in texts:\n",
    "  tokens=[]\n",
    "  raw=nltk.wordpunct_tokenize(text)\n",
    "  for token in raw:\n",
    "    tokens.append(token)\n",
    "    all_tokens.append(tokens)\n",
    "\n",
    "# Import and fit the model with data\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec(all_tokens)\n",
    "\n",
    "\n",
    "# Visualizing the word embedding\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6a5b3",
   "metadata": {},
   "source": [
    "<h2>represent the document using Doc2Vec model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28606f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.8027293e-03, -3.2461069e-03, -2.4526366e-03, -4.2229625e-03,\n",
       "       -3.9296835e-03, -2.9717141e-03, -3.4514227e-04, -1.6716033e-03,\n",
       "       -5.7986582e-04, -7.6274294e-04,  3.9847214e-03,  2.8663431e-03,\n",
       "       -3.7209971e-03, -7.8858233e-05,  2.8686349e-03, -4.2565214e-03,\n",
       "       -8.9871779e-04, -1.1194046e-03, -3.5259598e-03, -3.6086785e-03,\n",
       "        3.2026276e-03, -2.4940851e-03,  2.5354342e-03, -4.5621353e-03,\n",
       "        4.8431549e-03,  3.3726424e-03, -4.7325436e-03,  1.5779120e-03,\n",
       "        3.8904157e-03, -2.7358155e-03,  1.1344833e-03,  1.4870174e-03,\n",
       "       -4.6401988e-03,  2.9739169e-03, -2.6583597e-03,  3.4173273e-03,\n",
       "        3.4260156e-03, -2.1760764e-03,  3.8987377e-03, -9.6183445e-04,\n",
       "        9.3810074e-04, -4.8425794e-03,  2.5676538e-03, -2.7769606e-03,\n",
       "        2.3812284e-03,  8.5696980e-04, -3.1764999e-03,  3.9130524e-03,\n",
       "       -1.7898601e-03,  5.7824660e-04, -9.7676890e-04,  3.1566741e-03,\n",
       "        2.7983819e-04,  5.4023275e-04,  9.9991891e-04,  4.5082201e-03,\n",
       "        7.4235094e-04,  3.5623086e-03, -5.1529764e-04, -1.6012483e-03,\n",
       "        1.8577802e-03, -3.8954492e-03,  2.9573899e-03,  4.9311463e-03,\n",
       "       -2.6643632e-03,  4.5728395e-03,  4.9268454e-03,  2.6617181e-03,\n",
       "        1.0866446e-03, -6.2275049e-04,  1.5959160e-03, -7.6053233e-04,\n",
       "        2.8873465e-03,  1.1250771e-03, -2.6021996e-03,  3.7330628e-04,\n",
       "       -5.7283900e-04, -1.4361218e-04, -2.4702812e-03, -2.1894743e-04,\n",
       "        2.3612408e-03,  5.0300602e-03,  2.2049830e-03, -3.5582418e-03,\n",
       "       -3.6813589e-03,  2.4539960e-04, -3.3349330e-03,  6.8059919e-04,\n",
       "        4.8419074e-03, -3.9132242e-03,  1.7217997e-03,  1.5533924e-03,\n",
       "        2.8638463e-03, -3.2785942e-03, -1.8900158e-03, -1.2469495e-03,\n",
       "       -3.0630988e-03,  3.3072548e-03, -1.6719680e-03,  3.5100670e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts= [\" Photography is an excellent hobby to pursue \",\n",
    "        \" Photographers usually develop patience, calmnesss\"\n",
    "        \" You can try Photography with any good mobile too\"]\n",
    "from gensim.models import Doc2Vec\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "my_data = list(tagged_document(all_tokens))\n",
    "model=Doc2Vec(my_data)\n",
    "\n",
    "model.infer_vector(['photography','is','an',' excellent ','hobby ','to',' pursue '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0147f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.8027293e-03, -3.2461069e-03, -2.4526366e-03, -4.2229625e-03,\n",
       "       -3.9296835e-03, -2.9717141e-03, -3.4514227e-04, -1.6716033e-03,\n",
       "       -5.7986582e-04, -7.6274294e-04,  3.9847214e-03,  2.8663431e-03,\n",
       "       -3.7209971e-03, -7.8858233e-05,  2.8686349e-03, -4.2565214e-03,\n",
       "       -8.9871779e-04, -1.1194046e-03, -3.5259598e-03, -3.6086785e-03,\n",
       "        3.2026276e-03, -2.4940851e-03,  2.5354342e-03, -4.5621353e-03,\n",
       "        4.8431549e-03,  3.3726424e-03, -4.7325436e-03,  1.5779120e-03,\n",
       "        3.8904157e-03, -2.7358155e-03,  1.1344833e-03,  1.4870174e-03,\n",
       "       -4.6401988e-03,  2.9739169e-03, -2.6583597e-03,  3.4173273e-03,\n",
       "        3.4260156e-03, -2.1760764e-03,  3.8987377e-03, -9.6183445e-04,\n",
       "        9.3810074e-04, -4.8425794e-03,  2.5676538e-03, -2.7769606e-03,\n",
       "        2.3812284e-03,  8.5696980e-04, -3.1764999e-03,  3.9130524e-03,\n",
       "       -1.7898601e-03,  5.7824660e-04, -9.7676890e-04,  3.1566741e-03,\n",
       "        2.7983819e-04,  5.4023275e-04,  9.9991891e-04,  4.5082201e-03,\n",
       "        7.4235094e-04,  3.5623086e-03, -5.1529764e-04, -1.6012483e-03,\n",
       "        1.8577802e-03, -3.8954492e-03,  2.9573899e-03,  4.9311463e-03,\n",
       "       -2.6643632e-03,  4.5728395e-03,  4.9268454e-03,  2.6617181e-03,\n",
       "        1.0866446e-03, -6.2275049e-04,  1.5959160e-03, -7.6053233e-04,\n",
       "        2.8873465e-03,  1.1250771e-03, -2.6021996e-03,  3.7330628e-04,\n",
       "       -5.7283900e-04, -1.4361218e-04, -2.4702812e-03, -2.1894743e-04,\n",
       "        2.3612408e-03,  5.0300602e-03,  2.2049830e-03, -3.5582418e-03,\n",
       "       -3.6813589e-03,  2.4539960e-04, -3.3349330e-03,  6.8059919e-04,\n",
       "        4.8419074e-03, -3.9132242e-03,  1.7217997e-03,  1.5533924e-03,\n",
       "        2.8638463e-03, -3.2785942e-03, -1.8900158e-03, -1.2469495e-03,\n",
       "       -3.0630988e-03,  3.3072548e-03, -1.6719680e-03,  3.5100670e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts= [\" sad\"]\n",
    "from gensim.models import Doc2Vec\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "my_data = list(tagged_document(all_tokens))\n",
    "model=Doc2Vec(my_data)\n",
    "\n",
    "model.infer_vector(['photography','is','an',' excellent ','hobby ','to',' pursue '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae9180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
