{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea22fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9ad3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ee3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f4736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538dadf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81be2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40997633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ecddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d219ec90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m Text\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m tokens\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[43mtext\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(token)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "Text=(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe270b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "week\n",
      ",\n",
      "the\n",
      "University\n",
      "of\n",
      "Cambridge\n",
      "shared\n",
      "its\n",
      "own\n",
      "research\n",
      "that\n",
      "shows\n",
      "if\n",
      "everyone\n",
      "wears\n",
      "a\n",
      "mask\n",
      "outside\n",
      "home\n",
      ",\n",
      "dreaded\n",
      "‘\n",
      "second\n",
      "wave\n",
      "’\n",
      "of\n",
      "the\n",
      "pandemic\n",
      "can\n",
      "be\n",
      "avoided\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text=(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3226815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hind\n",
      "hh\n",
      "HOUSNI\n",
      "tes\n",
      ".\n",
      ",\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(\"Hind hh HOUSNI tes . , test\")\n",
    "for token in tokens :\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "952b30f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives.',\n",
       " 'Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others.',\n",
       " 'Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels?',\n",
       " 'Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves.',\n",
       " 'This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g.',\n",
       " 'radio, movies, television, the internet, mobiles) and the zeitgeist (e.g.',\n",
       " 'cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens.',\n",
       " 'Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18493d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello,i'm not free this after noon ,lert's go tomorrow\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello,i'm not free this after noon ,lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7424f55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello.i'm not free this after noon .lert's go tomorrow\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello.i'm not free this after noon .lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a368c747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello.', \"i'm not free this after noon.\", \"lert's go tomorrow\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello. i'm not free this after noon. lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31a3c24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "text=\"I love spring season. I go hiking with my friends\"\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d0a126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe1bb538dc94ec69c16e6ef6d7af5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee90657c8644e7b7b2b77c7d92140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c0a4c7a2d4779896af0a723c4244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d50a178e70412a8a62c4461a9c8b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceb1f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3500, 2161, 1012, 1045, 2175, 13039, 2007, 2026, 2814, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] i love spring season. i go hiking with my friends [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer.encode(text)\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caf27992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 17666, 7570, 2271, 3490, 3231, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] hind housni test [SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer.encode(\"hind housni test\")\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883e4f0",
   "metadata": {},
   "source": [
    "<h2> tokenize text with stopwords </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30e62c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walter',\n",
       " 'feeling anxious',\n",
       " 'He',\n",
       " 'diagnosed today',\n",
       " 'He probably',\n",
       " 'best person I know']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\n",
    "stop_words_and_delims =['was','is','the',',','.','-','?']\n",
    "for r in stop_words_and_delims:\n",
    "    text=text.replace(r,'DELIM')\n",
    "WORDS = [t.strip() for t in text.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a: a not in [''], words))\n",
    "words_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35de7736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked beh',\n",
       " 'd',\n",
       " 'door',\n",
       " \"nd didn't like\",\n",
       " 'he s',\n",
       " 'w. The three-ye',\n",
       " 'r-old girl r',\n",
       " 'n down',\n",
       " 'be',\n",
       " 'ch',\n",
       " 's',\n",
       " 'kite flew beh',\n",
       " 'd her. R',\n",
       " 'ndom words',\n",
       " 'front',\n",
       " 'o',\n",
       " 'r r',\n",
       " 'ndom words cre',\n",
       " 'te',\n",
       " 'r',\n",
       " 'ndom sentence.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\"in\",\"in\",\"of\",\"a\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3810cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " 'door',\n",
       " \"nd didn't like\",\n",
       " 'he s',\n",
       " 'w. The three-ye',\n",
       " 'r-old girl r',\n",
       " 'n down',\n",
       " 'be',\n",
       " 'ch',\n",
       " 's',\n",
       " 'kite flew behind her. R',\n",
       " 'ndom words in front of o',\n",
       " 'r r',\n",
       " 'ndom words cre',\n",
       " 'te',\n",
       " 'r',\n",
       " 'ndom sentence.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\"a\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1784225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " \"door and didn't like\",\n",
       " 'he saw. The three-year-old girl ran down',\n",
       " 'beach as',\n",
       " 'kite flew behind her. Random words in front of o',\n",
       " 'r random words create a random sentence.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "353947c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " \"door and didn't like\",\n",
       " 'he saw',\n",
       " 'The three-year-old girl ran down',\n",
       " 'beach as',\n",
       " 'kite flew behind her',\n",
       " 'Random words in front of o',\n",
       " 'r random words create',\n",
       " 'a random sentence']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create. a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\".\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70579e0f",
   "metadata": {},
   "source": [
    "<h2>remove stop words in a text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a771939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outbreak coronavirus disease 2019 ( COVID-19 ) created global health crisis deep impact way perceive world everyday lives . Not rate contagion patterns transmission threatens sense agency , safety measures put place contain spread virus also require social distancing refraining inherently human , find solace company others . Within context physical threat , social physical distancing , well public alarm , ( ) role different mass media channels lives individual , social societal levels ? Mass media long recognized powerful forces shaping experience world . This recognition accompanied growing volume research , closely follows footsteps technological transformations ( e.g . radio , movies , television , internet , mobiles ) zeitgeist ( e.g . cold war , 9/11 , climate change ) attempt map mass media major impacts perceive , individuals citizens . Are media ( broadcast digital ) still able convey sense unity reaching large audiences , messages lost noisy crowd mass self-communication ?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "my_stopwords=set(stopwords.words('english'))\n",
    "new_tokens=[]\n",
    "text=\"\"\"the outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "all_tokens=nltk.word_tokenize(text)\n",
    "\n",
    "for token in all_tokens:\n",
    "  if token not in my_stopwords:\n",
    "    new_tokens.append(token)\n",
    "\n",
    "\n",
    "\" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48319284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The You bite lower jaw . I think I buy red car , I lease blue one . The mysterious diary records voice .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "stopw = set(stopwords.words('english'))\n",
    "new=[]\n",
    "test=\"The You bite up because of your lower jaw. I think I will buy the red car, or I will lease the blue one. The mysterious diary records the voice.\"\n",
    "tokens=nltk.word_tokenize(test)\n",
    "for token  in tokens :\n",
    " if token not in stopw:\n",
    "     new.append(token)\n",
    "        \n",
    "        \n",
    "\" \".join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb681398",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fead614",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m new_tokens\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m----> 6\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_stop\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     new_tokens\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(new_tokens)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_stop'"
     ]
    }
   ],
   "source": [
    "text=\"\"\"the outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "import nltk \n",
    "doc=(text)\n",
    "new_tokens=[]\n",
    "for token in doc:\n",
    "  if token.is_stop==False:\n",
    "    new_tokens.append(token.text)\n",
    "\n",
    "\" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c646791",
   "metadata": {},
   "source": [
    "<h2>remove punctuations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9193bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The match has concluded India has won the match Will we fin the finals too'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text=\"The match has concluded !!! India has won the match . Will we fin the finals too ? !\"\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efce261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tart lemonade quenched her thirst but not her longing She moved forward only because she trusted that the ending she now was going through must be followed by a new beginning Pair your designer cowboy hat with scuba gear for a memorable occasion'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"The tart lemonade quenched her thirst, but not her longing.She moved forward only because she trusted that the ending she now was going through must be followed by a new beginning.Pair your designer cowboy hat with scuba gear for a memorable occasion.\"\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(test)\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d87fd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The__tart__lemonade__quenched__her__thirst__but__not__her__longing__She__moved__forward__only__because__she__trusted__that__the__ending__she__now__was__going__through__must__be__followed__by__a__new__beginning__Pair__your__designer__cowboy__hat__with__scuba__gear__for__a__memorable__occasion'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"The tart lemonade quenched her thirst, but not her longing.She moved forward only because she trusted that the ending she now was going through must be followed by a new beginning.Pair your designer cowboy hat with scuba gear for a memorable occasion.\"\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(test)\n",
    "\"__\".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f38e8d",
   "metadata": {},
   "source": [
    "<h2>perform stemming</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1551a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc is an art . student should be taught danc as a subject in school . i danc in mani of my school function . some peopl are alway hesit to danc .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
    "from nltk.stem import PorterStemmer \n",
    "stemmer=PorterStemmer()\n",
    "stemmed_tokens=[]\n",
    "for token in nltk.word_tokenize(text):\n",
    "  stemmed_tokens.append(stemmer.stem(token))\n",
    "\n",
    "\" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46328a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sarah ran from the serial killer hold a jug of milk.it is n't difficult to do a handstand if you just stand on your hands.i love eat toast chees and tuna sandwich .\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"Sarah ran from the serial killer holding a jug of milk.It isn't difficult to do a handstand if you just stand on your hands.I love eating toasted cheese and tuna sandwiches.\"\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmed=[]\n",
    "for token in nltk.word_tokenize(test):\n",
    "    stemmed.append(stemmer.stem(token))\n",
    "    \n",
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11728da8",
   "metadata": {},
   "source": [
    "<h2>lemmatize a given text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a916354",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "text= \"Dancing is an art. Students should be taught dance as a subject in schools . I danced in many of my school function. Some people are always hesitating to dance.\"\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "\n",
    "lemmatized=[token.lemma_ for token in doc]\n",
    "\" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a2bbc",
   "metadata": {},
   "source": [
    "<h1>extract usernames from emails</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f2bea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potter709', 'elixir101', 'granger111', 'severus77']\n"
     ]
    }
   ],
   "source": [
    "text= \"The new registrations are potter709@gmail.com , elixir101@gmail.com. If you find any disruptions, kindly contact granger111@gamil.com or severus77@gamil.com \"\n",
    "import re \n",
    "usernames= re.findall('(\\S+)@', text)     \n",
    "print(usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e69bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hindhousni25', ',hindhousni304']\n"
     ]
    }
   ],
   "source": [
    "test=\"hindhousni25@gmail.com ,hindhousni304@gmail.com\"\n",
    "import re\n",
    "name=re.findall('(\\S+)@',test)\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8fb09",
   "metadata": {},
   "source": [
    "<h2>finding the most common words in the text excluding stopwords</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2aceaa",
   "metadata": {},
   "source": [
    "<h2>spell correction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d7e5d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHe is a gret person. He beleives in bod\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m      3\u001b[0m text\u001b[38;5;241m=\u001b[39mTextBlob(text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(text\u001b[38;5;241m.\u001b[39mcorrect())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "text=\"He is a gret person. He beleives in bod\"\n",
    "from textblob import TextBlob\n",
    "text=TextBlob(text)\n",
    "print(text.correct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a00a3e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textblob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtextblob\u001b[49m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textblob' is not defined"
     ]
    }
   ],
   "source": [
    "textblob.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a13187",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textblob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtextblob\u001b[49m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textblob' is not defined"
     ]
    }
   ],
   "source": [
    "textblob.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b384f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a great person. He believes in god\n"
     ]
    }
   ],
   "source": [
    "text=\"He is a great person. He believes in god\"\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "text=TextBlob(text)\n",
    "print(text.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84cba7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Felo my name is hind ad i m eighteen years old\n"
     ]
    }
   ],
   "source": [
    "test=\"Helo my name is hind ad i m eighteen years old\"\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "test=TextBlob(test)\n",
    "print(test.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce063728",
   "metadata": {},
   "source": [
    "<h1>tokenize tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bff63fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'goa',\n",
       " 'vaction',\n",
       " 'summervacation',\n",
       " 'Fancy',\n",
       " 'dinner',\n",
       " 'Beachbay',\n",
       " 'restro']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\" Having lots of fun #goa #vaction #summervacation. Fancy dinner @Beachbay restro :) \"\n",
    "import re\n",
    "text=re.sub(r'[^\\w]',' ',text)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer=TweetTokenizer()\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b97ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test  tweets  big\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test', 'tweets', 'big']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=\"#test #tweets #big\"\n",
    "import re\n",
    "test=re.sub(r'[^\\w]',' ',test)\n",
    "print(test)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer=TweetTokenizer()\n",
    "tokenizer.tokenize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12a248",
   "metadata": {},
   "source": [
    "<h1>extract all the nouns in a text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be67675",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e70be1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJames works at Microsoft. She lives in manchester and likes to play the flute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m nlp\u001b[38;5;241m=\u001b[39m\u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc\u001b[38;5;241m=\u001b[39mnlp(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "text=\"James works at Microsoft. She lives in manchester and likes to play the flute\"\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc98c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abd9fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m word2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterrible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m word3\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcellent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m spacy download en_core_web_lg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m nlp\u001b[38;5;241m=\u001b[39mspacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_lg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "word1=\"amazing\"\n",
    "word2=\"terrible\"\n",
    "word3=\"excellent\"\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_lg\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "token1=nlp(word1)\n",
    "token2=nlp(word2)\n",
    "token3=nlp(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b536f37f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vectorization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvectorization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vectorizer\n\u001b[0;32m      5\u001b[0m text1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaj Mahal is a tourist place in India\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m text2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreat Wall of China is a tourist place in china\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vectorization'"
     ]
    }
   ],
   "source": [
    "import vectorize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from vectorization import Vectorizer\n",
    "text1='Taj Mahal is a tourist place in India'\n",
    "text2='Great Wall of China is a tourist place in china'\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "matrix=vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323bd6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e44304",
   "metadata": {},
   "source": [
    "<h2>find the cosine similarity of two documents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27391f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.45584231]\n",
      " [0.45584231 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "text1='Taj Mahal is a tourist place in India'\n",
    "text2='Great Wall of China is a tourist place in china'\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f4a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "text1=\"HOUSNI hind\"\n",
    "text2=\"HOUSNI hind\"\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e55340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "text1=\"HOUSNI hind \"\n",
    "text2=\"HOUSNI hind\"\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341ae01",
   "metadata": {},
   "source": [
    "<h2>find soft cosine similarity of documents</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d9dc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m     16\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary([simple_preprocess(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents])\n\u001b[0;32m     17\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m fasttext_model300\u001b[38;5;241m.\u001b[39msimilarity_matrix(dictionary, tfidf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, exponent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, nonzero_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'apps'"
     ]
    }
   ],
   "source": [
    "doc_soup = \"Soup is a primarily liquid food, generally served warm or hot (but may be cool or cold), that is made by combining ingredients of meat or vegetables with stock, juice, water, or another liquid. \"\n",
    "\n",
    "doc_noodles = \"Noodles are a staple food in many cultures. They are made from unleavened dough which is stretched, extruded, or rolled flat and cut into one of a variety of shapes.\"\n",
    "\n",
    "doc_dosa = \"Dosa is a type of pancake from the Indian subcontinent, made from a fermented batter. It is somewhat similar to a crepe in appearance. Its main ingredients are rice and black gram.\"\n",
    "\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
    "\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "import gensim\n",
    "from apps.document.models import Document\n",
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "sent_1 = dictionary.doc2bow(simple_preprocess(doc_trump))\n",
    "sent_2 = dictionary.doc2bow(simple_preprocess(doc_election))\n",
    "sent_3 = dictionary.doc2bow(simple_preprocess(doc_putin))\n",
    "sent_4 = dictionary.doc2bow(simple_preprocess(doc_soup))\n",
    "sent_5 = dictionary.doc2bow(simple_preprocess(doc_noodles))\n",
    "sent_6 = dictionary.doc2bow(simple_preprocess(doc_dosa))\n",
    "\n",
    "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1e229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 73.0% 1213.5/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "word2vec_model300.most_similar('amazing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581d2c4",
   "metadata": {},
   "source": [
    "<h2>compute Word mover distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81940c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
