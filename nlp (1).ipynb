{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea22fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9ad3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ee3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f4736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538dadf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81be2e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40997633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ecddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d219ec90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m Text\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m tokens\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[43mtext\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(token)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "Text=(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe270b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "week\n",
      ",\n",
      "the\n",
      "University\n",
      "of\n",
      "Cambridge\n",
      "shared\n",
      "its\n",
      "own\n",
      "research\n",
      "that\n",
      "shows\n",
      "if\n",
      "everyone\n",
      "wears\n",
      "a\n",
      "mask\n",
      "outside\n",
      "home\n",
      ",\n",
      "dreaded\n",
      "‘\n",
      "second\n",
      "wave\n",
      "’\n",
      "of\n",
      "the\n",
      "pandemic\n",
      "can\n",
      "be\n",
      "avoided\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text=(\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home,dreaded ‘second wave’ of the pandemic can be avoided.\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3226815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hind\n",
      "hh\n",
      "HOUSNI\n",
      "tes\n",
      ".\n",
      ",\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(\"Hind hh HOUSNI tes . , test\")\n",
    "for token in tokens :\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "952b30f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives.',\n",
       " 'Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others.',\n",
       " 'Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels?',\n",
       " 'Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves.',\n",
       " 'This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g.',\n",
       " 'radio, movies, television, the internet, mobiles) and the zeitgeist (e.g.',\n",
       " 'cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens.',\n",
       " 'Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"The outbreak of coronavirus disease 2019 (COVID-19) has created a global health crisis that has had a deep impact on the way we perceive our world and our everyday lives. Not only the rate of contagion and patterns of transmission threatens our sense of agency, but the safety measures put in place to contain the spread of the virus also require social distancing by refraining from doing what is inherently human, which is to find solace in the company of others. Within this context of physical threat, social and physical distancing, as well as public alarm, what has been (and can be) the role of the different mass media channels in our lives on individual, social and societal levels? Mass media have long been recognized as powerful forces shaping how we experience the world and ourselves. This recognition is accompanied by a growing volume of research, that closely follows the footsteps of technological transformations (e.g. radio, movies, television, the internet, mobiles) and the zeitgeist (e.g. cold war, 9/11, climate change) in an attempt to map mass media major impacts on how we perceive ourselves, both as individuals and citizens. Are media (broadcast and digital) still able to convey a sense of unity reaching large audiences, or are messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18493d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello,i'm not free this after noon ,lert's go tomorrow\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello,i'm not free this after noon ,lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7424f55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hello.i'm not free this after noon .lert's go tomorrow\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello.i'm not free this after noon .lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a368c747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello.', \"i'm not free this after noon.\", \"lert's go tomorrow\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=(\"hello. i'm not free this after noon. lert's go tomorrow\")\n",
    "nltk.sent_tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31a3c24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "text=\"I love spring season. I go hiking with my friends\"\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85d0a126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe1bb538dc94ec69c16e6ef6d7af5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee90657c8644e7b7b2b77c7d92140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6c0a4c7a2d4779896af0a723c4244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d50a178e70412a8a62c4461a9c8b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceb1f14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3500, 2161, 1012, 1045, 2175, 13039, 2007, 2026, 2814, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] i love spring season. i go hiking with my friends [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer.encode(text)\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caf27992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 17666, 7570, 2271, 3490, 3231, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] hind housni test [SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer.encode(\"hind housni test\")\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883e4f0",
   "metadata": {},
   "source": [
    "<h2> tokenize text with stopwords </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30e62c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walter',\n",
       " 'feeling anxious',\n",
       " 'He',\n",
       " 'diagnosed today',\n",
       " 'He probably',\n",
       " 'best person I know']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\n",
    "stop_words_and_delims =['was','is','the',',','.','-','?']\n",
    "for r in stop_words_and_delims:\n",
    "    text=text.replace(r,'DELIM')\n",
    "WORDS = [t.strip() for t in text.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a: a not in [''], words))\n",
    "words_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35de7736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked beh',\n",
       " 'd',\n",
       " 'door',\n",
       " \"nd didn't like\",\n",
       " 'he s',\n",
       " 'w. The three-ye',\n",
       " 'r-old girl r',\n",
       " 'n down',\n",
       " 'be',\n",
       " 'ch',\n",
       " 's',\n",
       " 'kite flew beh',\n",
       " 'd her. R',\n",
       " 'ndom words',\n",
       " 'front',\n",
       " 'o',\n",
       " 'r r',\n",
       " 'ndom words cre',\n",
       " 'te',\n",
       " 'r',\n",
       " 'ndom sentence.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\"in\",\"in\",\"of\",\"a\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3810cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " 'door',\n",
       " \"nd didn't like\",\n",
       " 'he s',\n",
       " 'w. The three-ye',\n",
       " 'r-old girl r',\n",
       " 'n down',\n",
       " 'be',\n",
       " 'ch',\n",
       " 's',\n",
       " 'kite flew behind her. R',\n",
       " 'ndom words in front of o',\n",
       " 'r r',\n",
       " 'ndom words cre',\n",
       " 'te',\n",
       " 'r',\n",
       " 'ndom sentence.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\"a\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1784225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " \"door and didn't like\",\n",
       " 'he saw. The three-year-old girl ran down',\n",
       " 'beach as',\n",
       " 'kite flew behind her. Random words in front of o',\n",
       " 'r random words create a random sentence.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "353947c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looked behind',\n",
       " \"door and didn't like\",\n",
       " 'he saw',\n",
       " 'The three-year-old girl ran down',\n",
       " 'beach as',\n",
       " 'kite flew behind her',\n",
       " 'Random words in front of o',\n",
       " 'r random words create',\n",
       " 'a random sentence']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random=\"He looked behind the door and didn't like what he saw. The three-year-old girl ran down the beach as the kite flew behind her. Random words in front of other random words create. a random sentence.\"\n",
    "stop_words=[\"He\",\"the\",\"what\",\".\"]\n",
    "for r in stop_words:\n",
    "    random = random.replace(r,'DELIM')\n",
    "words = [t.strip() for t in random.split('DELIM')]\n",
    "words_filtered = list(filter(lambda a : a not in [''],words))\n",
    "words_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70579e0f",
   "metadata": {},
   "source": [
    "<h2>remove stop words in a text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25864c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
